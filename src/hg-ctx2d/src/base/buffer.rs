use std::{
    borrow::Cow,
    ops::Range,
    sync::{mpsc, Mutex},
};

use hg_utils::hash::FxHashMap;
use smallvec::SmallVec;
use thunderdome::{Arena, Index};

use crate::{PositionedVecWriter, StreamWrite, StreamWriter};

// === DynamicBufferManager === //

#[derive(Debug, Copy, Clone, Hash, Eq, PartialEq)]
pub enum DynamicBufferMode {
    /// A buffer which supports all dynamic buffer manipulation operations but requires the
    /// operations to be aligned to the [`wgpu::COPY_BUFFER_ALIGNMENT`] copy alignment limit.
    Blind,

    /// A buffer which supports all dynamic buffer manipulation operations at arbitrary offsets at
    /// the expense of maintaining a CPU-local copy of the data.
    Mirrored,

    /// Automatically choses between `DynamicBlind` and `DynamicMirrored` depending on the minimum
    /// alignment needed for dynamic accesses.
    Auto { align: wgpu::BufferAddress },
}

#[derive(Debug, Copy, Clone, Hash, Eq, PartialEq, Ord, PartialOrd)]
pub struct DynamicBufferHandle(pub Index);

#[derive(Debug)]
pub struct DynamicBufferManager {
    device: wgpu::Device,
    belt: ChunkStagingBelt,
    buffers: Arena<DynamicBuffer>,
}

#[derive(Debug)]
struct DynamicBuffer {
    label: Option<Cow<'static, str>>,
    usages: wgpu::BufferUsages,

    /// The active length of the buffer w.r.t un-flushed modifications.
    target_len: wgpu::BufferAddress,

    /// The used length of the `buffer` that was previously uploaded or `0` if no buffer has yet
    /// been mapped. If the buffer is truncated, this value will be reduced to ensure that truncated
    /// data from the `buffer` is not unnecessarily included in any resized buffers.
    buffer_len: wgpu::BufferAddress,

    /// The current instance buffer generated by the brush. This buffer persists between frames and
    /// is only `None` if we haven't yet created a backing buffer.
    buffer: Option<wgpu::Buffer>,

    /// A chunk-based staging belt writer to the target buffer.
    writer: ChunkBufferWriter,

    /// A CPU-local copy of the data contained by the buffer.
    local_copy: Option<Vec<u8>>,
}

impl DynamicBufferManager {
    pub fn new(device: wgpu::Device) -> Self {
        Self {
            device,
            belt: ChunkStagingBelt::new(1 << 16),
            buffers: Arena::new(),
        }
    }

    pub fn create(
        &mut self,
        label: Option<Cow<'static, str>>,
        mode: DynamicBufferMode,
        usages: wgpu::BufferUsages,
    ) -> DynamicBufferHandle {
        let needs_local_copy = match mode {
            DynamicBufferMode::Blind => false,
            DynamicBufferMode::Mirrored => true,
            DynamicBufferMode::Auto { align } => align < wgpu::COPY_BUFFER_ALIGNMENT,
        };

        DynamicBufferHandle(self.buffers.insert(DynamicBuffer {
            label,
            usages,
            target_len: 0,
            buffer_len: 0,
            buffer: None,
            writer: ChunkBufferWriter::new(),
            local_copy: needs_local_copy.then_some(Vec::new()),
        }))
    }

    pub fn destroy(&mut self, buffer: DynamicBufferHandle) {
        let Some(mut state) = self.buffers.remove(buffer.0) else {
            return;
        };

        state.writer.release(&mut self.belt);
    }

    pub fn len(&self, buffer: DynamicBufferHandle) -> wgpu::BufferAddress {
        self.buffers[buffer.0].target_len
    }

    pub fn write(
        &mut self,
        buffer: DynamicBufferHandle,
        offset: wgpu::BufferAddress,
        data: &impl StreamWrite,
    ) {
        let state = &mut self.buffers[buffer.0];

        // Update the CPU copy if necessary
        if let Some(local_copy) = &mut state.local_copy {
            data.write_to(&mut PositionedVecWriter {
                target: local_copy,
                start: offset as usize,
            });
        }

        // Use a staging buffer to write into a buffer directly.
        let written = state
            .writer
            .write(&self.device, &mut self.belt, state.target_len, data);

        state.target_len = state.target_len.max(offset + written);
    }

    pub fn push(&mut self, buffer: DynamicBufferHandle, data: &impl StreamWrite) {
        self.write(buffer, self.len(buffer), data);
    }

    pub fn truncate(&mut self, buffer: DynamicBufferHandle, to: wgpu::BufferAddress) {
        let state = &mut self.buffers[buffer.0];

        // Adjust virtual length
        assert!(to <= state.target_len);
        state.target_len = to;

        // Adjust buffer length, inserting placeholder chunks to keep indexing consistent.
        state.buffer_len = state.buffer_len.min(to);
    }

    pub fn clear(&mut self, buffer: DynamicBufferHandle) {
        self.truncate(buffer, 0);
    }

    pub fn flush(&mut self, encoder: &mut wgpu::CommandEncoder) {
        for (_handle, state) in &mut self.buffers {
            // Resize the underlying buffer to accommodate our data.
            if state
                .buffer
                .as_ref()
                .is_none_or(|buffer| buffer.size() < state.target_len)
            {
                let new_size = state.target_len; // TODO: Better sizing strategy
                let new_buffer = self.device.create_buffer(&wgpu::BufferDescriptor {
                    label: state.label.as_deref(),
                    size: new_size,
                    usage: state.usages,
                    mapped_at_creation: false,
                });

                if let Some(old_buffer) = &state.buffer {
                    encoder.copy_buffer_to_buffer(&old_buffer, 0, &new_buffer, 0, state.buffer_len);
                }

                state.buffer = Some(new_buffer);
            }

            let Some(buffer) = &state.buffer else {
                unreachable!();
            };

            // Flush writes
            state
                .writer
                .flush(&mut self.belt, encoder, buffer, state.local_copy.as_deref());

            state.buffer_len = state.target_len;
        }
    }
}

// === Utilities === //

#[derive(Debug)]
pub struct ChunkStagingBelt {
    chunk_size: wgpu::BufferAddress,
    free_chunks: Vec<wgpu::Buffer>,
    free_chunk_recv: Mutex<mpsc::Receiver<wgpu::Buffer>>,
    free_chunk_send: mpsc::Sender<wgpu::Buffer>,
}

impl ChunkStagingBelt {
    pub fn new(chunk_size: wgpu::BufferAddress) -> Self {
        let (free_chunk_send, free_chunk_recv) = mpsc::channel();
        Self {
            chunk_size,
            free_chunks: Vec::new(),
            free_chunk_recv: Mutex::new(free_chunk_recv),
            free_chunk_send,
        }
    }

    pub fn chunk_size(&self) -> wgpu::BufferAddress {
        self.chunk_size
    }

    pub fn acquire(&mut self, device: &wgpu::Device) -> wgpu::Buffer {
        while let Ok(chunk) = self.free_chunk_recv.get_mut().unwrap().try_recv() {
            self.free_chunks.push(chunk);
        }

        if let Some(chunk) = self.free_chunks.pop() {
            return chunk;
        }

        let buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("DynamicBuffer staging"),
            size: self.chunk_size,
            usage: wgpu::BufferUsages::COPY_SRC | wgpu::BufferUsages::MAP_WRITE,
            mapped_at_creation: true,
        });

        buffer
    }

    pub fn release(&mut self, buffer: wgpu::Buffer) {
        let tx = self.free_chunk_send.clone();

        buffer
            .clone()
            .slice(..)
            .map_async(wgpu::MapMode::Write, move |res| {
                if let Err(err) = res {
                    tracing::warn!("failed to remap {buffer:?}: {err}");
                    buffer.destroy();
                    return;
                }

                if let Err(err) = tx.send(buffer) {
                    err.0.destroy();
                }
            });
    }
}

#[derive(Debug, Default)]
pub struct ChunkBufferWriter {
    /// Maps chunk base address to a buffer containing the chunk's staged data.
    chunks: FxHashMap<wgpu::BufferAddress, CbwChunk>,
}

#[derive(Debug)]
struct CbwChunk {
    buffer: wgpu::Buffer,
    intervals: SmallVec<[Range<u32>; 1]>,
}

impl ChunkBufferWriter {
    pub fn new() -> Self {
        Self::default()
    }

    pub fn write(
        &mut self,
        device: &wgpu::Device,
        belt: &mut ChunkStagingBelt,
        offset: wgpu::BufferAddress,
        data: &impl StreamWrite,
    ) -> wgpu::BufferAddress {
        struct WriteTarget<'a> {
            writer: &'a mut ChunkBufferWriter,
            device: &'a wgpu::Device,
            belt: &'a mut ChunkStagingBelt,
            offset: wgpu::BufferAddress,
            total_written: wgpu::BufferAddress,
        }

        impl StreamWriter for WriteTarget<'_> {
            fn write(&mut self, mut data: &[u8]) {
                while !data.is_empty() {
                    // Determine the chunk for this current transaction.
                    let chunk_base = self.offset / self.belt.chunk_size() * self.belt.chunk_size();

                    let chunk = self
                        .writer
                        .chunks
                        .entry(chunk_base)
                        .or_insert_with(|| CbwChunk {
                            buffer: self.belt.acquire(self.device),
                            intervals: SmallVec::new(),
                        });

                    // Copy part of the data into the buffer.
                    let mut view = chunk.buffer.slice(..).get_mapped_range_mut();
                    let written = view.len().min(data.len());
                    view[..written].copy_from_slice(&data[..written]);
                    data = &data[written..];

                    // Advance the `offset` and the `data` slice in parallel.
                    let start = (self.offset % self.belt.chunk_size()) as u32;
                    self.offset += written as wgpu::BufferAddress;
                    let end = (self.offset % self.belt.chunk_size()) as u32;

                    // Delete all intervals that overlap with us, extending our interval as we go.
                    let mut to_insert = start..end;
                    let mut i = 0;

                    while i < chunk.intervals.len() {
                        let other = &chunk.intervals[i];

                        if !(other.start <= to_insert.start && to_insert.end <= other.end) {
                            i += 1;
                            continue;
                        }

                        to_insert.start = to_insert.start.min(other.start);
                        to_insert.end = to_insert.end.max(other.end);

                        chunk.intervals.swap_remove(i);
                        // (do not increment `i` because we have a new element in this position)
                    }

                    chunk.intervals.push(to_insert);
                }
            }
        }

        let mut target = WriteTarget {
            writer: self,
            device,
            belt,
            offset,
            total_written: 0,
        };
        data.write_to(&mut target);
        target.total_written
    }

    pub fn flush(
        &mut self,
        belt: &mut ChunkStagingBelt,
        encoder: &mut wgpu::CommandEncoder,
        target: &wgpu::Buffer,
        local_copy: Option<&[u8]>,
    ) {
        for (chunk_base, chunk) in self.chunks.drain() {
            // TODO: Coalesce intervals with small gaps between them if we have a local copy
            //  available to us? In the same step, align buffer with `local_copy` if unaligned.

            for interval in chunk.intervals {
                encoder.copy_buffer_to_buffer(
                    &chunk.buffer,
                    interval.start.into(),
                    target,
                    chunk_base + u64::from(interval.start),
                    u64::from(interval.end - interval.start),
                );
            }

            belt.release(chunk.buffer);
        }
    }

    pub fn release(&mut self, belt: &mut ChunkStagingBelt) {
        for (_addr, chunk) in self.chunks.drain() {
            belt.release(chunk.buffer);
        }
    }
}
